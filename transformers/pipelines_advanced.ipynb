{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HF Transformers 核心模块学习：Pipelines 进阶\n",
    "\n",
    "我们已经学习了 Pipeline API 针对各类任务的基本使用。\n",
    "\n",
    "实际上，在 Transformers 库内部实现中，Pipeline 作为管理：`原始文本-输入Token IDs-模型推理-输出概率-生成结果` 的流水线抽象，背后串联了 Transformers 库的核心模块 `Tokenizer`和 `Models`。\n",
    "\n",
    "![](docs/images/pipeline_advanced.png)\n",
    "\n",
    "下面我们开始结合大语言模型（在 Transformers 中也是一种特定任务）学习：\n",
    "\n",
    "- 使用 Pipeline 如何与现代的大语言模型结合，以完成各类下游任务\n",
    "- 使用 Tokenizer 编解码文本\n",
    "- 使用 Models 加载和保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Pipeline 调用大语言模型\n",
    "\n",
    "### Language Modeling\n",
    "\n",
    "语言建模是一项预测文本序列中的单词的任务。它已经成为非常流行的自然语言处理任务，因为预训练的语言模型可以用于许多其他下游任务的微调。最近，对大型语言模型（LLMs）产生了很大兴趣，这些模型展示了零或少量样本学习能力。这意味着该模型可以解决其未经明确训练过的任务！虽然语言模型可用于生成流畅且令人信服的文本，但需要小心使用，因为文本可能并不总是准确无误。\n",
    "\n",
    "通过理论篇学习，我们了解到有两种典型的语言模型：\n",
    "\n",
    "- 自回归：模型目标是预测序列中的下一个 Token（文本），训练时对下文进行了掩码。如：GPT-3。\n",
    "- 自编码：模型目标是理解上下文后，补全句子中丢失/掩码的 Token（文本）。如：BERT。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 GPT-2 实现文本生成\n",
    "\n",
    "![](docs/images/gpt2.png)\n",
    "\n",
    "模型主页：https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zk/miniconda3/envs/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/zk/miniconda3/envs/peft/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/Users/zk/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hugging Face is a community-based open-source platform for machine learning. This has been used by the likes of Quora Group, Hacker News Groups, Ecosyn, S2E, and many more to describe what it could potentially mean'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n",
    "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置文本生成返回条数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are very smart\"\n",
    "generator = pipeline(task=\"text-generation\", model=\"gpt2\", num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You are very smart.\"\\n\\nFor me, she is too. I am so sure of that, that this is something that doesn\\'t end yet. I don\\'t think she has an heir. She\\'s a princess, and they can\\'t have'},\n",
       " {'generated_text': 'You are very smart and very funny.\"\\n\\n\\'You will soon be considered for president of the United States\\' by President Barack Obama\\n\\nShe added: \"This story, I\\'ve never heard of before, it\\'s just unbelievable and something I'},\n",
       " {'generated_text': 'You are very smart and I trust you!\"\\n\\n\"Yes, well then—that\\'s what I should say!\" he quipped.\\n\\n\"Is he not a man of this world?\"\\n\\n\"Yes, but no, I\\'ve'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You are very smart – and I think this has helped me,\" said Thomas. While a recent survey by CBS News found that 48% approve of the way Obama is handling his job, only 10% approve.\\n\\nObama has been in the White'},\n",
       " {'generated_text': 'You are very smart, very smart people, you are not the only ones who can understand why these people are so happy. It is a beautiful world.\"\\n\\nBut the other problem, he says, is not being educated, but is being ignored'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置文本生成最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You are very smart, are your hands so good, why not make a movie'},\n",
       " {'generated_text': 'You are very smart and incredibly well supported in the community and I was very nervous'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(prompt, num_return_sequences=2, max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 BERT-Base-Chinese 实现中文补全\n",
    "\n",
    "![](docs/images/bert-base-chinese.png)\n",
    "\n",
    "模型主页：https://huggingface.co/bert-base-chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(task=\"fill-mask\", model=\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.6703642010688782,\n",
       "   'token': 679,\n",
       "   'token_str': '不',\n",
       "   'sequence': '[CLS] 人 民 是 不 可 [MASK] [MASK] 的 [SEP]'}],\n",
       " [{'score': 0.07147283107042313,\n",
       "   'token': 5543,\n",
       "   'token_str': '能',\n",
       "   'sequence': '[CLS] 人 民 是 [MASK] 可 能 [MASK] 的 [SEP]'}],\n",
       " [{'score': 0.07442499697208405,\n",
       "   'token': 5375,\n",
       "   'token_str': '缺',\n",
       "   'sequence': '[CLS] 人 民 是 [MASK] 可 [MASK] 缺 的 [SEP]'}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"人民是[MASK]可[MASK][MASK]的\"\n",
    "\n",
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置文本补全的条数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.759693443775177,\n",
       "  'token': 8043,\n",
       "  'token_str': '？',\n",
       "  'sequence': '美 国 的 首 都 是 ？'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"美国的首都是[MASK]\"\n",
    "\n",
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9911921620368958,\n",
       "  'token': 3791,\n",
       "  'token_str': '法',\n",
       "  'sequence': '巴 黎 是 法 国 的 首 都 。'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"巴黎是[MASK]国的首都。\"\n",
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.759693443775177,\n",
       "  'token': 8043,\n",
       "  'token_str': '？',\n",
       "  'sequence': '美 国 的 首 都 是 ？'},\n",
       " {'score': 0.21126750111579895,\n",
       "  'token': 511,\n",
       "  'token_str': '。',\n",
       "  'sequence': '美 国 的 首 都 是 。'},\n",
       " {'score': 0.02683430165052414,\n",
       "  'token': 8013,\n",
       "  'token_str': '！',\n",
       "  'sequence': '美 国 的 首 都 是 ！'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"美国的首都是[MASK]\"\n",
    "fill_mask(text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.09838863462209702,\n",
       "   'token': 1525,\n",
       "   'token_str': '哪',\n",
       "   'sequence': '[CLS] 美 国 的 首 都 是 哪 [MASK] [SEP]'}],\n",
       " [{'score': 0.7994449138641357,\n",
       "   'token': 8043,\n",
       "   'token_str': '？',\n",
       "   'sequence': '[CLS] 美 国 的 首 都 是 [MASK] ？ [SEP]'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"美国的首都是[MASK][MASK]\"\n",
    "\n",
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 思考：sequence 中出现的 [CLS] 和 [SEP] 是什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 AutoClass 高效管理 `Tokenizer` 和 `Model`\n",
    "\n",
    "通常，您想要使用的模型（网络架构）可以从您提供给 `from_pretrained()` 方法的预训练模型的名称或路径中推测出来。\n",
    "\n",
    "AutoClasses就是为了帮助用户完成这个工作，以便根据`预训练权重/配置文件/词汇表的名称/路径自动检索相关模型`。\n",
    "\n",
    "比如手动加载`bert-base-chinese`模型以及对应的 `tokenizer` 方法如下：\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "```\n",
    "\n",
    "以下是我们实际操作和演示：\n",
    "\n",
    "### 使用 `from_pretrained` 方法加载指定 Model 和 Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-chinese\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 BERT Tokenizer 编码文本\n",
    "\n",
    "编码 (Encoding) 过程包含两个步骤：\n",
    "\n",
    "- 分词：使用分词器按某种策略将文本切分为 tokens；\n",
    "- 映射：将 tokens 转化为对应的 token IDs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中', '国', '的', '首', '都', '是', '北', '京']\n"
     ]
    }
   ],
   "source": [
    "# 第一步：分词\n",
    "sequence = \"中国的首都是北京\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二步：映射\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[704, 1744, 4638, 7674, 6963, 3221, 1266, 776]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 Tokenizer.encode 方法端到端处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids_e2e = tokenizer.encode(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_e2e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 思考：为什么前后新增了 101 和 102？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中 国 的 首 都 是 北 京'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 中 国 的 首 都 是 北 京 [SEP]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids_e2e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编解码多段文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_batch = [\"美国的首都是华盛顿特区\", \"中国的首都是北京\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids_batch = tokenizer.encode(sequence_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 美 国 的 首 都 是 华 盛 顿 特 区 [SEP] 中 国 的 首 都 是 北 京 [SEP]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](docs/images/bert_pretrain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实操建议：直接使用 tokenizer.\\_\\_call\\_\\_ 方法完成文本编码 + 特殊编码补全\n",
    "\n",
    "编码后返回结果：\n",
    "\n",
    "```json\n",
    "input_ids: token_ids\n",
    "token_type_ids: token_id 归属的句子编号\n",
    "attention_mask: 指示哪些token需要被关注（注意力机制）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "embedding_batch = tokenizer(\"美国的首都是华盛顿特区\", \"中国的首都是北京\")\n",
    "print(embedding_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102]\n",
      "\n",
      "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 优化下输出结构\n",
    "for key, value in embedding_batch.items():\n",
    "    print(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加新 Token\n",
    "\n",
    "当出现了词表或嵌入空间中不存在的新Token，需要使用 Tokenizer 将其添加到词表中。 Transformers 库提供了两种不同方法：\n",
    "\n",
    "- add_tokens: 添加常规的正文文本 Token，以追加（append）的方式添加到词表末尾。\n",
    "- add_special_tokens: 添加特殊用途的 Token，优先在已有特殊词表中选择（`bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token`）。如果预定义均不满足，则都添加到`additional_special_tokens`。\n",
    "\n",
    "#### 添加常规 Token\n",
    "\n",
    "先查看已有词表，确保新添加的 Token 不在词表中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##閃: 20329\n",
      "要: 6206\n",
      "淮: 3917\n",
      "sports: 10646\n",
      "疑: 4542\n",
      "193: 10185\n",
      "##倉: 13999\n",
      "##艰: 18737\n",
      "碳: 4823\n",
      "hotel: 8462\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# 使用 islice 查看词表部分内容\n",
    "for key, value in islice(tokenizer.vocab.items(), 10):\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = [\"天干\", \"地支\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将集合作差结果添加到词表中\n",
    "new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'地支', '天干'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(list(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21130"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新增加了2个Token，词表总数由 21128 增加到 21130\n",
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 添加特殊Token（审慎操作）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_special_token = {\"sep_token\": \"NEW_SPECIAL_TOKEN\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens(new_special_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21131"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新增加了1个特殊Token，词表总数由 21128 增加到 21131\n",
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 `save_pretrained` 方法保存指定 Model 和 Tokenizer \n",
    "\n",
    "借助 `AutoClass` 的设计理念，保存 Model 和 Tokenizer 的方法也相当高效便捷。\n",
    "\n",
    "假设我们对`bert-base-chinese`模型以及对应的 `tokenizer` 做了修改，并更名为`new-bert-base-chinese`，方法如下：\n",
    "\n",
    "```python\n",
    "tokenizer.save_pretrained(\"./models/new-bert-base-chinese\")\n",
    "model.save_pretrained(\"./models/new-bert-base-chinese\")\n",
    "```\n",
    "\n",
    "保存 Tokenizer 会在指定路径下创建以下文件：\n",
    "- tokenizer.json: Tokenizer 元数据文件；\n",
    "- special_tokens_map.json: 特殊字符映射关系配置文件；\n",
    "- tokenizer_config.json: Tokenizer 基础配置文件，存储构建 Tokenizer 需要的参数；\n",
    "- vocab.txt: 词表文件；\n",
    "- added_tokens.json: 单独存放新增 Tokens 的配置文件。\n",
    "\n",
    "保存 Model 会在指定路径下创建以下文件：\n",
    "- config.json：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；\n",
    "- pytorch_model.bin：又称为 state dictionary，存储模型的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/new-bert-base-chinese/tokenizer_config.json',\n",
       " './models/new-bert-base-chinese/special_tokens_map.json',\n",
       " './models/new-bert-base-chinese/vocab.txt',\n",
       " './models/new-bert-base-chinese/added_tokens.json',\n",
       " './models/new-bert-base-chinese/tokenizer.json')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./models/new-bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/new-bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
